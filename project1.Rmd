---
title: "STAT 27850 Project 1"
header-includes:
  - \usepackage{soul}
  - \usepackage{xcolor}
  - \usepackage{titlesec}
  - \DeclareMathOperator*{\argmin}{argmin}
output:
  pdf_document: default
geometry:
- margin=1 in
- top=  0.9cm
fontsize: 10pt 
---
```{r global-options, include=FALSE}
knitr::opts_chunk$set(
        fig.width=5, fig.height=3.4,
        fig.path='Figs/', fig.align='center',
        warning=FALSE, message=FALSE,
        strip.white=TRUE, cache=TRUE)
```
\newcommand{\h}[1]{\colorbox{yellow}{$\displaystyle #1$}} 
\titlespacing{\title}{0pt}{\parskip}{-\parskip}    
\vspace{-12.5truemm} 
 
\begin{center}
Christopher Tang
\end{center}


**1**
```{r, eval = TRUE}
library(dplyr)
library(ggplot2)

load("bikedata.RData")
# format covariates into a single data frame for easy viewing
bikenum <- as.data.frame(bikenum); day_of_week <- as.data.frame(day_of_week)
duration <- as.data.frame(duration); member <- as.data.frame(member)
station_end <- as.data.frame(station_end); station_start <- as.data.frame(station_start)
days_since_Jan1_2010 <- as.data.frame(days_since_Jan1_2010)
data <- cbind(station_start,station_end,duration,member,bikenum,day_of_week,days_since_Jan1_2010 )
data$ridename <- paste(station_start,' - ',station_end)
data <- add_count(data,station_start,station_end)
```

```{r}
# add weather covariates
weather <- read.csv("WeatherData.csv")
data_weather <- left_join(data,weather,by='days_since_Jan1_2010')
```

   \emph{(a)}     
       


```{r}
# top ride
df <- data_weather[data_weather$station_start==31104 & data_weather$station_end==31106,] %>%
  group_by(days_since_Jan1_2010) %>%
  summarize(mean_daily_duration = mean(duration), count=n(), day_of_week=day_of_week, member=member)

ggplot(data=df, aes(x=days_since_Jan1_2010, y=mean_daily_duration)) +
  geom_line() + ylim(c(0,1000))
```
The average daily duration oscillates over time very drastically, suggesting people happen to ride longer bike trips on certain days of the week.


## Subset data and run regression on each ride and save p-vals + coefficients
```{r}
# routes without infrequent rides
routes <- data_weather[data_weather$n > 100,][,c('station_start', 'station_end')] %>% distinct() # mention problematic
route_pvals <- rep(NA, nrow(routes))
route_coef <- rep(NA, nrow(routes))


fit_list <- vector(mode = "list", length = nrow(routes))
for (r in 1:nrow(routes)) {
  # remove outlier rides > 3 sd away from route mean
  route_data <- data_weather[data_weather$station_start==routes[r,]$station_start & data_weather$station_end==routes[r,]$station_end,]
  upper_cutoff <- mean(route_data$duration) + 3 * sd(route_data$duration) # mention data dependent
  lower_cutoff <- mean(route_data$duration) - 3 * sd(route_data$duration)
  route_data <- route_data[route_data$duration < upper_cutoff & route_data$duration > lower_cutoff,]
  
  # run regression of route 
  fit <- lm(duration ~ days_since_Jan1_2010 + day_of_week + member + TMAX + PRCP, data=route_data) # linear regression assumptions
  pval <- summary(fit)$coefficients[,4]['days_since_Jan1_2010']
  coef <- summary(fit)$coefficients[,1]['days_since_Jan1_2010']
  route_pvals[r] <- pval
  route_coef[r] <- coef
  fit_list[[r]] <- fit
}
```

```{r}
#save('fit_list', 'route_pvals', 'route_coef', file="data/regression_out.RData")
```

## Bonferroni significance table
```{r}
# routes w/ p val lower boferonni at alpha= 0.05
get_sig_table <- function(sig_ind) {
  sig_routes <- routes[sig_ind,]
  sig_fits <- fit_list[sig_ind]
  sig_pval <- as.numeric(lapply(sig_fits, function(x) {summary(x)$coefficients[,4]['days_since_Jan1_2010']}))
  sig_coef <- as.numeric(lapply(sig_fits, function(x) {summary(x)$coefficients[,1]['days_since_Jan1_2010']}))
  sig_sd <- as.numeric(lapply(sig_fits, function(x) {summary(x)$coefficients[,2]['days_since_Jan1_2010']}))
  sig_df <- cbind(sig_routes, sig_coef, sig_sd, sig_pval)
  names(sig_df) <- c('station_start', 'station_end', 'Beta_hat', 'std_error', 'pvalue')
  sig_df <- sig_df[order(sig_df$pvalue),]
}

sig_bonferonni <- get_sig_table(which(route_pvals < 0.05/length(route_pvals)))
print(sig_bonferonni)
```
## BH Significance Table
```{r}
# same table as above but for BH
runBH <- function(P, alpha, gamma, runStorey=TRUE) {
  n <- length(P)
  P_squiggle <- rep(NA, n)
  # calculate Storey modified P
  if (runStorey) {
    pi0 <- sum(P > gamma) / (n * (1-gamma))
    pi0 <- if (pi0 > 1) 1 else pi0 # cap pi0 at 1
    P_squiggle <- P * pi0
  } else {
    P_squiggle <- P
  }
  reject_vec <- rep(0, n)
  # run BH
  k_hat <- 0
  P_squiggle_sort <- sort(P_squiggle, index.return=T)
  for (k in 1:n) {
    p_index <- P_squiggle_sort$ix[k]
    if (P_squiggle_sort$x[k] <= alpha*k/n && P[p_index] <= gamma) {
      k_hat <- k
    }
  }
  if (k_hat != 0) {
    reject_indices <- P_squiggle_sort$ix[1:k_hat]
    reject_vec[reject_indices] <- 1
  }
  reject_vec
}

# run standard BH
route_rejects_standard <- runBH(route_pvals, alpha=0.05, gamma=0.5, runStorey=F)
sig_BH <- get_sig_table(which(route_rejects_standard==1))
print(sig_BH)

# run on our pvalues w/ Storey correction
route_rejects_storey <- runBH(route_pvals, alpha=0.05, gamma=0.5, runStorey=T)
sig_storeyBH <- get_sig_table(which(route_rejects_storey==1))
print(sig_storeyBH)
```



```{r}
# plot most significant route duration over time with duration corrected for covariates
mindex <- which.min(route_pvals)
route <- routes[mindex,]
route_data <- data_weather[data_weather$station_start==route$station_start & data_weather$station_end==route$station_end,]
upper_cutoff <- mean(route_data$duration) + 3 * sd(route_data$duration)
lower_cutoff <- mean(route_data$duration) - 3 * sd(route_data$duration)
route_data <- route_data[route_data$duration < upper_cutoff & route_data$duration > lower_cutoff,]

route_data['duration_resid'] <- resid(lm(duration ~ day_of_week + member + TMAX + PRCP, data=route_data))

df <- route_data %>%
  group_by(days_since_Jan1_2010) %>%
  summarize(mean_daily_duration_residualized = mean(duration_resid), count=n(), day_of_week=day_of_week, member=member)

ggplot(data=df, aes(x=days_since_Jan1_2010, y=mean_daily_duration_residualized)) +
  geom_line() + geom_smooth(method='lm', formula=y~x) #+ ylim(c(0,5000))

```


```{r}
# find source for geographical data (lat & long) regarding stations
locData <- read.csv("https://gist.githubusercontent.com/since1968/e51c0f3d95e67bf49f74/raw/37a6c381df119b7463c8fd33fdfaa06427d9794f/bikeStations.csv")
colnames(stations) <- c("terminalName","locations")

#take those with matching terminalName
newStations<-merge(x=stations,y=locData,by="terminalName",all.x=TRUE)
newStations<-newStations[, c("terminalName", "locations", "lat", "long")]
```

After the merge, some of the locations do not match (excluding those wherein the order of the street names were simply switched such as "Pentagon City Metro / 12th & S Hayes St" vs "12th & Hayes St /  Pentagon City Metro"). For these pairs of locations that do not match at all, we resorted to verifying the latitude and longitude for the location listed in `stations`, a data object provided from the Group Project 1 instructions. Latitude and longitude for these locations were gathered from inputting location names into Google Maps.

The 5 terminals with different locations are: 31000, 31500, 31302, 31609, 31239
```{r}
newStations[1,3] <- 38.85979; newStations[1,4] <- -77.05357
newStations[78,3] <- 38.90567; newStations[78,4] <- -77.04120 
newStations[1,3] <- 38.93465; newStations[1,4] <--77.07246
newStations[1,3] <- 38.91930; newStations[1,4] <- -77.00056
newStations[1,3] <- 38.87863; newStations[1,4] <- -77.02283
```


## Permutation test
if null, permutation doesnt change dist
- if confounding, permutation can change -> deal with this

       
