---
title: "STAT 27850 Project 1"
header-includes:
  - \usepackage{soul}
  - \usepackage{xcolor}
  - \usepackage{titlesec}
  - \DeclareMathOperator*{\argmin}{argmin}
output:
  pdf_document: default
geometry:
- margin=1 in
- top=  0.9cm
fontsize: 10pt 
---
```{r global-options, include=FALSE}
knitr::opts_chunk$set(
        fig.width=5, fig.height=3.4,
        fig.path='Figs/', fig.align='center',
        warning=FALSE, message=FALSE,
        strip.white=TRUE, cache=TRUE)
```
\newcommand{\h}[1]{\colorbox{yellow}{$\displaystyle #1$}} 
\titlespacing{\title}{0pt}{\parskip}{-\parskip}    
\vspace{-12.5truemm} 
 
\begin{center}
Christopher Tang
\end{center}


**1**
```{r, eval = TRUE}
library(dplyr)
library(ggplot2)

load("bikedata.RData")
# format covariates into a single data frame for easy viewing
bikenum <- as.data.frame(bikenum); day_of_week <- as.data.frame(day_of_week)
duration <- as.data.frame(duration); member <- as.data.frame(member)
station_end <- as.data.frame(station_end); station_start <- as.data.frame(station_start)
days_since_Jan1_2010 <- as.data.frame(days_since_Jan1_2010)
data <- cbind(station_start,station_end,duration,member,bikenum,day_of_week,days_since_Jan1_2010 )
data$ridename <- paste(station_start,' - ',station_end)
data <- add_count(data,station_start,station_end)
```

```{r}
# add weather covariates
weather <- read.csv("WeatherData.csv")
data_weather <- left_join(data,weather,by='days_since_Jan1_2010')
```

   \emph{(a)}     
       


```{r}
# top ride
df <- data_weather[data_weather$station_start==31104 & data_weather$station_end==31106,] %>%
  group_by(days_since_Jan1_2010) %>%
  summarize(mean_daily_duration = mean(duration), count=n(), day_of_week=day_of_week, member=member)

ggplot(data=df, aes(x=days_since_Jan1_2010, y=mean_daily_duration)) +
  geom_line() + ylim(c(0,1000))
```



## Subset data and run regression on each ride and save p-vals + coefficients
```{r}
# function to remove outlier rides > 3 sd away from route mean
get_route_data <- function(start_station, end_station) {
  route_data <- data_weather[data_weather$station_start==start_station & data_weather$station_end==end_station,]
  upper_cutoff <- mean(route_data$duration) + 3 * sd(route_data$duration) # mention data dependent
  lower_cutoff <- mean(route_data$duration) - 3 * sd(route_data$duration)
  route_data <- route_data[route_data$duration < upper_cutoff & route_data$duration > lower_cutoff,]
  return(route_data)
}

# routes without infrequent rides
routes <- data_weather[data_weather$n > 100,][,c('station_start', 'station_end')] %>% distinct() # mention problematic
route_pvals <- rep(NA, nrow(routes))
route_coef <- rep(NA, nrow(routes))
route_pvals_inverse <- rep(NA, nrow(routes))
route_coef_inverse <- rep(NA, nrow(routes))

fit_list <- vector(mode = "list", length = nrow(routes))
fit_list_inverse <- vector(mode = "list", length = nrow(routes))
for (r in 1:nrow(routes)) {
  # remove outlier rides > 3 sd away from route mean
  route_data <- get_route_data(routes[r,]$station_start, routes[r,]$station_end)
  
  # run regression of route 
  fit <- lm(duration ~ days_since_Jan1_2010 + day_of_week + member + TMAX + PRCP, data=route_data)
  pval <- summary(fit)$coefficients[,4]['days_since_Jan1_2010']
  coef <- summary(fit)$coefficients[,1]['days_since_Jan1_2010']
  route_pvals[r] <- pval
  route_coef[r] <- coef
  fit_list[[r]] <- fit
  
  fit_inverse <- lm((duration^-1) ~ days_since_Jan1_2010 + day_of_week + member + TMAX + PRCP, data=route_data)
  route_pvals_inverse[r] <- summary(fit_inverse)$coefficients[,4]['days_since_Jan1_2010']
  route_coef_inverse[r] <- summary(fit_inverse)$coefficients[,1]['days_since_Jan1_2010']
  fit_list_inverse[[r]] <- fit_inverse
}
```

## Plots to test regression assumptions on randomly sampled data
```{r}
set.seed(123)
samples <- round(runif(3, 0, nrow(routes)))
# plot standard fit 1
par(mfrow = c(2, 2))

# 3 randomly sampled routes
plot(fit_list[[samples[1]]])
plot(fit_list_inverse[[samples[1]]])
route_data <- get_route_data(routes[samples[1],]$station_start, routes[samples[1],]$station_end)
a <- fastDummies::dummy_cols(route_data, select_columns="day_of_week")
ggcorrplot::ggcorrplot(cor(a %>% 
                             select(duration, days_since_Jan1_2010, member, day_of_week_Monday, 
                                    day_of_week_Tuesday, day_of_week_Wednesday, day_of_week_Thursday,
                                    day_of_week_Friday, day_of_week_Saturday, day_of_week_Sunday, PRCP, TMAX)))

plot(fit_list[[samples[2]]])
plot(fit_list_inverse[[samples[2]]])
route_data <- get_route_data(routes[samples[2],]$station_start, routes[samples[2],]$station_end)
a <- fastDummies::dummy_cols(route_data, select_columns="day_of_week")
ggcorrplot::ggcorrplot(cor(a %>% 
                             select(duration, days_since_Jan1_2010, member, day_of_week_Monday, 
                                    day_of_week_Tuesday, day_of_week_Wednesday, day_of_week_Thursday,
                                    day_of_week_Friday, day_of_week_Saturday, day_of_week_Sunday, PRCP, TMAX)))

plot(fit_list[[samples[3]]])
plot(fit_list_inverse[[samples[3]]])
route_data <- get_route_data(routes[samples[3],]$station_start, routes[samples[3],]$station_end)
a <- fastDummies::dummy_cols(route_data, select_columns="day_of_week")
ggcorrplot::ggcorrplot(cor(a %>% 
                             select(duration, days_since_Jan1_2010, member, day_of_week_Monday, 
                                    day_of_week_Tuesday, day_of_week_Wednesday, day_of_week_Thursday,
                                    day_of_week_Friday, day_of_week_Saturday, day_of_week_Sunday, PRCP, TMAX)))

# route 1
plot(fit_list[[1]])
plot(fit_list_inverse[[1]])
route_data <- get_route_data(routes[1,]$station_start, routes[1,]$station_end)
a <- fastDummies::dummy_cols(route_data, select_columns="day_of_week")
ggcorrplot::ggcorrplot(cor(a %>% 
                             select(duration, days_since_Jan1_2010, member, day_of_week_Monday, 
                                    day_of_week_Tuesday, day_of_week_Wednesday, day_of_week_Thursday,
                                    day_of_week_Friday, day_of_week_Saturday, day_of_week_Sunday, PRCP, TMAX)))
```

## Residuals autocorrelation with DW statistic across all models fitted
```{r}
DW_standard <- as.numeric(lapply(1:nrow(routes), function(r) {
  car::durbinWatsonTest(fit_list[[r]])$p
}))

DW_inverse <- as.numeric(lapply(1:nrow(routes), function(r) {
  car::durbinWatsonTest(fit_list_inverse[[r]])$p
}))

```
```{r}
df <- as.data.frame(DW_standard)
colnames(df) <- c("DW_p_val")
p1 <- ggplot(data=df, aes(x=DW_p_val)) + geom_histogram() + ylim(c(0,1750))

df <- as.data.frame(DW_inverse)
colnames(df) <- c("DW_p_val")
p2 <- ggplot(data=df, aes(x=DW_p_val)) + geom_histogram() + ylim(c(0,1750))

cowplot::plot_grid(p1, p2, labels = c("Standard", "Inverse"))
```


## Score test for heteroskedasticity across all models fitted
```{r}
# note: iid assumption violated, maybe try other test like Breusch-Pagan
score_standard <- as.numeric(lapply(1:nrow(routes), function(r) {
  olsrr::ols_test_score(fit_list[[r]])$p
}))

score_inverse <- as.numeric(lapply(1:nrow(routes), function(r) {
  olsrr::ols_test_score(fit_list_inverse[[r]])$p
}))

```
```{r}
df <- as.data.frame(score_standard)
colnames(df) <- c("score_p_val")
p1 <- ggplot(data=df, aes(x=score_p_val)) + geom_histogram() + ylim(c(0,2800))

df <- as.data.frame(score_inverse)
colnames(df) <- c("score_p_val")
p2 <- ggplot(data=df, aes(x=score_p_val)) + geom_histogram() + ylim(c(0,2800))

cowplot::plot_grid(p1, p2, labels = c("Standard", "Inverse"))
```

## Testing Normality with Shapiro Wilks test
```{r}
shapiro_standard <- as.numeric(lapply(1:nrow(routes), function(r) {
  shapiro.test(fit_list[[r]]$residuals)$p.value
}))

shapiro_inverse <- as.numeric(lapply(1:nrow(routes), function(r) {
  shapiro.test(fit_list_inverse[[r]]$residuals)$p.value
}))

```
```{r}
df <- as.data.frame(shapiro_standard)
colnames(df) <- c("shapiro_p_val")
p1 <- ggplot(data=df, aes(x=shapiro_p_val)) + geom_histogram() + ylim(c(0,3500))

df <- as.data.frame(shapiro_inverse)
colnames(df) <- c("shapiro_p_val")
p2 <- ggplot(data=df, aes(x=shapiro_p_val)) + geom_histogram() + ylim(c(0,3500))

cowplot::plot_grid(p1, p2, labels = c("Standard", "Inverse"))
```



```{r}
# save('fit_list', 'route_pvals', 'route_coef', file="data/regression_out.RData")
# save('fit_list_inverse', 'route_pvals_inverse', 'route_coef_inverse', file="data/regression_out_inverse.RData")
```

## Bonferroni significance table
```{r}
# routes w/ p val lower boferonni at alpha= 0.05
get_sig_table <- function(sig_ind, inverse=F) {
  sig_routes <- routes[sig_ind,]
  sig_fits <- fit_list[sig_ind]
  if (inverse) {
    sig_fits <- fit_list_inverse[sig_ind]
  }
  sig_pval <- as.numeric(lapply(sig_fits, function(x) {summary(x)$coefficients[,4]['days_since_Jan1_2010']}))
  sig_coef <- as.numeric(lapply(sig_fits, function(x) {summary(x)$coefficients[,1]['days_since_Jan1_2010']}))
  sig_sd <- as.numeric(lapply(sig_fits, function(x) {summary(x)$coefficients[,2]['days_since_Jan1_2010']}))
  sig_df <- cbind(sig_routes, sig_coef, sig_sd, sig_pval)
  names(sig_df) <- c('station_start', 'station_end', 'Beta_hat', 'std_error', 'pvalue')
  sig_df <- sig_df[order(sig_df$pvalue),]
}

# standard fit 
sig_bonferonni <- get_sig_table(which(route_pvals < 0.05/length(route_pvals)))
print(sig_bonferonni)

# inverse fit
sig_bonferonni <- get_sig_table(which(route_pvals_inverse < 0.05/length(route_pvals_inverse)), inverse=T)
print(sig_bonferonni)
```

## BH Significance Table
```{r}
# same table as above but for BH
runBH <- function(P, alpha, gamma, runStorey=TRUE, return_p=F) {
  n <- length(P)
  P_squiggle <- rep(NA, n)
  # calculate Storey modified P
  if (runStorey) {
    pi0 <- sum(P > gamma) / (n * (1-gamma))
    pi0 <- if (pi0 > 1) 1 else pi0 # cap pi0 at 1
    P_squiggle <- P * pi0
  } else {
    P_squiggle <- P
  }
  reject_vec <- rep(0, n)
  # run BH
  k_hat <- 0
  P_squiggle_sort <- sort(P_squiggle, index.return=T)
  for (k in 1:n) {
    p_index <- P_squiggle_sort$ix[k]
    if (P_squiggle_sort$x[k] <= alpha*k/n && P[p_index] <= gamma) {
      k_hat <- k
    }
  }
  if (k_hat != 0) {
    reject_indices <- P_squiggle_sort$ix[1:k_hat]
    reject_vec[reject_indices] <- 1
  }
  if (!return_p) {
    return(reject_vec)
  }
  else {
    return(P_squiggle)
  }
}

# run standard BH
route_rejects_standard <- runBH(route_pvals, alpha=0.05, gamma=0.5, runStorey=F)
route_rejects_standard_pval <- runBH(route_pvals, alpha=0.05, gamma=0.5, runStorey=F, return_p=T)
sig_BH <- get_sig_table(which(route_rejects_standard==1))
print(sig_BH)

# run on our pvalues w/ Storey correction
route_rejects_storey <- runBH(route_pvals, alpha=0.05, gamma=0.5, runStorey=T)
route_rejects_storey_pval <- runBH(route_pvals, alpha=0.05, gamma=0.5, runStorey=T, return_p=T)
sig_storeyBH <- get_sig_table(which(route_rejects_storey==1))
print(sig_storeyBH)

# run standard BH on inverse transformation
route_rejects_standard_inverse <- runBH(route_pvals_inverse, alpha=0.05, gamma=0.5, runStorey=F)
route_rejects_standard_inverse_pval <- runBH(route_pvals_inverse, alpha=0.05, gamma=0.5, runStorey=F, return_p=T)
sig_BH_inverse <- get_sig_table(which(route_rejects_standard_inverse==1), inverse=T)
print(sig_BH_inverse)

# run on our pvalues w/ Storey correction on inverse information
route_rejects_storey_inverse <- runBH(route_pvals_inverse, alpha=0.05, gamma=0.5, runStorey=T)
route_rejects_storey_inverse_pval <- runBH(route_pvals_inverse, alpha=0.05, gamma=0.5, runStorey=T, return_p=T)
sig_storeyBH_inverse <- get_sig_table(which(route_rejects_storey_inverse==1), inverse=T)
print(sig_storeyBH_inverse)
```

```{r}
#save('routes', 'route_rejects_standard', 'route_rejects_storey', 'route_rejects_standard_inverse', 'route_rejects_storey_inverse', 'sig_BH', 'sig_storeyBH', 'sig_BH_inverse', 'sig_storeyBH_inverse', file="data/rejections.RData")
```



```{r}
plot_corrected_route <- function(route_index, inverse=F) {
  route <- routes[mindex,]
  route_data <- data_weather[data_weather$station_start==route$station_start & data_weather$station_end==route$station_end,]
  upper_cutoff <- mean(route_data$duration) + 3 * sd(route_data$duration)
  lower_cutoff <- mean(route_data$duration) - 3 * sd(route_data$duration)
  route_data <- route_data[route_data$duration < upper_cutoff & route_data$duration > lower_cutoff,]
  
  if (!inverse) {
    route_data['duration_resid'] <- resid(lm(duration ~ day_of_week + member + TMAX + PRCP, data=route_data))
  } else {
    route_data['duration_resid'] <- resid(lm((duration^-1) ~ day_of_week + member + TMAX + PRCP, data=route_data))
  }
  
  df <- route_data %>%
    group_by(days_since_Jan1_2010) %>%
    summarize(mean_daily_duration_residualized = mean(duration_resid), count=n(), day_of_week=day_of_week, member=member)
 
  return(ggplot(data=df, aes(x=days_since_Jan1_2010, y=mean_daily_duration_residualized)) +
    geom_line() + geom_smooth(method='lm', formula=y~x))
}

# plot most significant route duration over time with duration corrected for covariates
mindex <- which.min(route_pvals)
p <- plot_corrected_route(mindex)
print(p)

mindex_inverse <- which.min(route_pvals_inverse)
p <- plot_corrected_route(mindex_inverse, inverse=T)
print(p)

```


```{r}
# find source for geographical data (lat & long) regarding stations
locData <- read.csv("https://gist.githubusercontent.com/since1968/e51c0f3d95e67bf49f74/raw/37a6c381df119b7463c8fd33fdfaa06427d9794f/bikeStations.csv")
colnames(stations) <- c("terminalName","locations")

#take those with matching terminalName
newStations<-merge(x=stations,y=locData,by="terminalName",all.x=TRUE)
newStations<-newStations[, c("terminalName", "locations", "lat", "long")]
```

After the merge, some of the locations do not match (excluding those wherein the order of the street names were simply switched such as "Pentagon City Metro / 12th & S Hayes St" vs "12th & Hayes St /  Pentagon City Metro"). For these pairs of locations that do not match at all, we resorted to verifying the latitude and longitude for the location listed in `stations`, a data object provided from the Group Project 1 instructions. Latitude and longitude for these locations were gathered from inputting location names into Google Maps.

The 5 terminals with different locations are: 31000, 31500, 31302, 31609, 31239
```{r}
newStations[1,3] <- 38.85979; newStations[1,4] <- -77.05357
newStations[78,3] <- 38.90567; newStations[78,4] <- -77.04120 
newStations[1,3] <- 38.93465; newStations[1,4] <--77.07246
newStations[1,3] <- 38.91930; newStations[1,4] <- -77.00056
newStations[1,3] <- 38.87863; newStations[1,4] <- -77.02283
```

## Permutation test
if null, permutation doesnt change dist
- if confounding, permutation can change
- permute duration within groups of member, day of week, binned temperature, binned precipitation

## Create binned temp and precip quartiles
```{r}
data_weather['TMAX_quartile'] <- ntile(data_weather$TMAX, 4)
data_weather['PRCP_none'] <- as.numeric(data_weather$PRCP == 0)
data_weather['PRCP_little'] <- as.numeric(data_weather$PRCP < 0.3 & data_weather$PRCP > 0)
data_weather['PRCP_big'] <- as.numeric(data_weather$PRCP > 0.3)
data_weather <- fastDummies::dummy_cols(data_weather, select_columns=c("TMAX_quartile"))
```


## For every route, permute durations by covariate group and see if distribution changes
```{r}
set.seed(123)
pvals_permtest <- rep(NA, nrow(routes))
n_perm <- 1500

nCores <- 8
# permutation test on correlation between duration and days since
for (r in 1:nrow(routes)) {
  route_data <- get_route_data(routes[r,]$station_start, routes[r,]$station_end)
  # observed test statistic (pearson correlation)
  T_obs <- cor(route_data$duration, route_data$days_since_Jan1_2010)
  neg <- if (T_obs > 0) F else T
  # permute to get estimates of null dist
  T_perms <- rep(NA, n_perm)
  for (i in 1:n_perm) {
    duration_perm <- sample(nrow(route_data), nrow(route_data), )

    # permute by covariates
    route_data_perm <- route_data %>%
      group_by(member, day_of_week, TMAX_quartile, PRCP_none, PRCP_little, PRCP_big) %>%
      mutate(duration_perm = if(n() == 1) duration else sample(duration, n()))

    T_perms[i] <- cor(route_data_perm$duration_perm, route_data$days_since_Jan1_2010)
  }
  
  # T_perms <- parallel::mclapply(1:n_perm, function(i) {
  #   duration_perm <- sample(nrow(route_data), nrow(route_data), )
  #   # permute by covariates
  #   route_data_perm <- route_data %>%
  #     group_by(member, day_of_week, TMAX_quartile, PRCP_none, PRCP_little, PRCP_big) %>%
  #     mutate(duration_perm = if(n() == 1) duration else sample(duration, n()))
  # 
  #   return(cor(route_data_perm$duration_perm, route_data$days_since_Jan1_2010))
  # },
  # mc.cores=parallel::detectCores()-2)
  
  pvals_permtest[r] <- if (neg) (1 + sum(T_perms < T_obs)) / (1 + n_perm) else (1 + sum(T_perms > T_obs)) / (1 + n_perm)
  if (r %% 500 == 0) {
    print(r)
  }
}
```


## BH corrections for perm test
```{r}
# run standard BH
perm_rejects_standard <- runBH(pvals_permtest, alpha=0.05, gamma=0.5, runStorey=F)
perm_rejects_standard_pval <- runBH(pvals_permtest, alpha=0.05, gamma=0.5, runStorey=F, return_p=T)

# run on our pvalues w/ Storey correction
perm_rejects_storey <- runBH(pvals_permtest, alpha=0.05, gamma=0.5, runStorey=T)
perm_rejects_storey_pval <- runBH(pvals_permtest, alpha=0.05, gamma=0.5, runStorey=T, return_p=T)
```

```{r}
# save to file
#save('pvals_permtest', 'perm_rejects_standard', 'perm_rejects_standard_pval', 'perm_rejects_storey', 'perm_rejects_storey_pval', file="data/perm_rejections.RData")
load('data/perm_rejections_150.RData')
```



```{r}
cors_routes <- rep(NA, nrow(routes))
for (r in 1:nrow(routes)) {
  route_data <- get_route_data(routes[r,]$station_start, routes[r,]$station_end)
  cors_routes[r] <- cor(route_data$duration, route_data$days_since_Jan1_2010)
}
```
```{r}
# average correlation of storey significant routes
mean(cors_routes[which(perm_rejects_storey == 1)])
```






       
