---
title: "STAT 27850 Project 1"
header-includes:
  - \usepackage{soul}
  - \usepackage{xcolor}
  - \usepackage{titlesec}
  - \DeclareMathOperator*{\argmin}{argmin}
output:
  pdf_document: default
geometry:
- margin=1 in
- top=  0.9cm
fontsize: 10pt 
---
```{r global-options, include=FALSE}
knitr::opts_chunk$set(
        fig.width=5, fig.height=3.4,
        fig.path='Figs/', fig.align='center',
        warning=FALSE, message=FALSE,
        strip.white=TRUE, cache=TRUE)
```
\newcommand{\h}[1]{\colorbox{yellow}{$\displaystyle #1$}} 
\titlespacing{\title}{0pt}{\parskip}{-\parskip}    
\vspace{-12.5truemm} 
 
\begin{center}
Christopher Tang
\end{center}


**1**
```{r, eval = TRUE}
library(dplyr)
library(ggplot2)

load("bikedata.RData")
# format covariates into a single data frame for easy viewing
bikenum <- as.data.frame(bikenum); day_of_week <- as.data.frame(day_of_week)
duration <- as.data.frame(duration); member <- as.data.frame(member)
station_end <- as.data.frame(station_end); station_start <- as.data.frame(station_start)
days_since_Jan1_2010 <- as.data.frame(days_since_Jan1_2010)
data <- cbind(station_start,station_end,duration,member,bikenum,day_of_week,days_since_Jan1_2010 )
data$ridename <- paste(station_start,' - ',station_end)
data <- add_count(data,station_start,station_end)
```

```{r}
# add weather covariates
weather <- read.csv("WeatherData.csv")
data_weather <- left_join(data,weather,by='days_since_Jan1_2010')
```

   \emph{(a)}     
       


```{r}
# top ride
df <- data_weather[data_weather$station_start==31104 & data_weather$station_end==31106,] %>%
  group_by(days_since_Jan1_2010) %>%
  summarize(mean_daily_duration = mean(duration), count=n(), day_of_week=day_of_week, member=member)

ggplot(data=df, aes(x=days_since_Jan1_2010, y=mean_daily_duration)) +
  geom_line() + ylim(c(0,1000))
```
The average daily duration oscillates over time very drastically, suggesting people happen to ride longer bike trips on certain days of the week.


## Subset data and run regression on each ride and save p-vals + coefficients
```{r}
# remove outlier rides > 3 sd away from route mean
get_route_data <- function(start_station, end_station) {
  route_data <- data_weather[data_weather$station_start==start_station & data_weather$station_end==end_station,]
  upper_cutoff <- mean(route_data$duration) + 3 * sd(route_data$duration) # mention data dependent
  lower_cutoff <- mean(route_data$duration) - 3 * sd(route_data$duration)
  route_data <- route_data[route_data$duration < upper_cutoff & route_data$duration > lower_cutoff,]
  return(route_data)
}

# routes without infrequent rides
routes <- data_weather[data_weather$n > 100,][,c('station_start', 'station_end')] %>% distinct() # mention problematic
route_pvals <- rep(NA, nrow(routes))
route_coef <- rep(NA, nrow(routes))
route_pvals_inverse <- rep(NA, nrow(routes))
route_coef_inverse <- rep(NA, nrow(routes))

fit_list <- vector(mode = "list", length = nrow(routes))
fit_list_inverse <- vector(mode = "list", length = nrow(routes))
for (r in 1:nrow(routes)) {
  # remove outlier rides > 3 sd away from route mean
  route_data <- get_route_data(routes[r,]$station_start, routes[r,]$station_end)
  
  # run regression of route 
  fit <- lm(duration ~ days_since_Jan1_2010 + day_of_week + member + TMAX + PRCP, data=route_data)
  pval <- summary(fit)$coefficients[,4]['days_since_Jan1_2010']
  coef <- summary(fit)$coefficients[,1]['days_since_Jan1_2010']
  route_pvals[r] <- pval
  route_coef[r] <- coef
  fit_list[[r]] <- fit
  
  fit_inverse <- lm((duration^-1) ~ days_since_Jan1_2010 + day_of_week + member + TMAX + PRCP, data=route_data)
  route_pvals_inverse[r] <- summary(fit_inverse)$coefficients[,4]['days_since_Jan1_2010']
  route_coef_inverse[r] <- summary(fit_inverse)$coefficients[,1]['days_since_Jan1_2010']
  fit_list_inverse[[r]] <- fit_inverse
}
```

## Plotting regression assumptions on randomly sampled data
```{r}
# plot standard fit 1
par(mfrow = c(2, 2))
plot(fit_list[[1]])
plot(fit_list_inverse[[1]])

route_data <- get_route_data(routes[1,]$station_start, routes[1,]$station_end)
a <- fastDummies::dummy_cols(route_data, select_columns="day_of_week")
ggcorrplot::ggcorrplot(cor(a %>% 
                             select(duration, days_since_Jan1_2010, member, day_of_week_Monday, 
                                    day_of_week_Tuesday, day_of_week_Wednesday, day_of_week_Thursday,
                                    day_of_week_Friday, day_of_week_Saturday, day_of_week_Sunday, PRCP, TMAX)))
```

## Residuals autocorrelation with DW statistic across all models fitted
```{r}
DW_standard <- as.numeric(lapply(1:nrow(routes), function(r) {
  olsrr::ols_test_score(fit_list[[r]])$p
}))

DW_inverse <- as.numeric(lapply(1:nrow(routes), function(r) {
  olsrr::ols_test_score(fit_list_inverse[[r]])$p
}))

```
```{r}

```


## Score test for heteroskedasticity across all models fitted
```{r}
score_standard <- as.numeric(lapply(1:nrow(routes), function(r) {
  olsrr::ols_test_score(fit_list[[r]])$p
}))

score_inverse <- as.numeric(lapply(1:nrow(routes), function(r) {
  olsrr::ols_test_score(fit_list_inverse[[r]])$p
}))

```
```{r}
par(mfrow=c(2,1))
df <- as.data.frame(score_standard)
colnames(df) <- c("score_p_val")
p1 <- ggplot(data=df, aes(x=score_p_val)) + geom_histogram() + ylim(c(0,2800))

df <- as.data.frame(score_inverse)
colnames(df) <- c("score_p_val")
p2 <- ggplot(data=df, aes(x=score_p_val)) + geom_histogram() + ylim(c(0,2800))

cowplot::plot_grid(p1, p2, labels = c("Standard", "Inverse"))
```

## Normality with Kolmogorov-Smirnov test
```{r}
# plot inverse fit 1
ks.test(fit_list_inverse[[1]]$residuals, "pnorm", mean=mean(fit_list_inverse[[1]]$residuals), sd=sd(fit_list_inverse[[1]]$residuals))

ks.test(fit_list[[1]]$residuals, "pnorm", mean=mean(fit_list[[1]]$residuals), sd=sd(fit_list[[1]]$residuals))

```
```{r}

```



```{r}
# save('fit_list', 'route_pvals', 'route_coef', file="data/regression_out.RData")
# save('fit_list_inverse', 'route_pvals_inverse', 'route_coef_inverse', file="data/regression_out_inverse.RData")
```

## Bonferroni significance table
```{r}
# routes w/ p val lower boferonni at alpha= 0.05
get_sig_table <- function(sig_ind, inverse=F) {
  sig_routes <- routes[sig_ind,]
  sig_fits <- fit_list[sig_ind]
  if (inverse) {
    sig_fits <- fit_list_inverse[sig_ind]
  }
  sig_pval <- as.numeric(lapply(sig_fits, function(x) {summary(x)$coefficients[,4]['days_since_Jan1_2010']}))
  sig_coef <- as.numeric(lapply(sig_fits, function(x) {summary(x)$coefficients[,1]['days_since_Jan1_2010']}))
  sig_sd <- as.numeric(lapply(sig_fits, function(x) {summary(x)$coefficients[,2]['days_since_Jan1_2010']}))
  sig_df <- cbind(sig_routes, sig_coef, sig_sd, sig_pval)
  names(sig_df) <- c('station_start', 'station_end', 'Beta_hat', 'std_error', 'pvalue')
  sig_df <- sig_df[order(sig_df$pvalue),]
}

# standard fit 
sig_bonferonni <- get_sig_table(which(route_pvals < 0.05/length(route_pvals)))
print(sig_bonferonni)

# inverse fit
sig_bonferonni <- get_sig_table(which(route_pvals_inverse < 0.05/length(route_pvals_inverse)), inverse=T)
print(sig_bonferonni)
```

## BH Significance Table
```{r}
# same table as above but for BH
runBH <- function(P, alpha, gamma, runStorey=TRUE) {
  n <- length(P)
  P_squiggle <- rep(NA, n)
  # calculate Storey modified P
  if (runStorey) {
    pi0 <- sum(P > gamma) / (n * (1-gamma))
    pi0 <- if (pi0 > 1) 1 else pi0 # cap pi0 at 1
    P_squiggle <- P * pi0
  } else {
    P_squiggle <- P
  }
  reject_vec <- rep(0, n)
  # run BH
  k_hat <- 0
  P_squiggle_sort <- sort(P_squiggle, index.return=T)
  for (k in 1:n) {
    p_index <- P_squiggle_sort$ix[k]
    if (P_squiggle_sort$x[k] <= alpha*k/n && P[p_index] <= gamma) {
      k_hat <- k
    }
  }
  if (k_hat != 0) {
    reject_indices <- P_squiggle_sort$ix[1:k_hat]
    reject_vec[reject_indices] <- 1
  }
  reject_vec
}

# run standard BH
route_rejects_standard <- runBH(route_pvals, alpha=0.05, gamma=0.5, runStorey=F)
sig_BH <- get_sig_table(which(route_rejects_standard==1))
print(sig_BH)

# run on our pvalues w/ Storey correction
route_rejects_storey <- runBH(route_pvals, alpha=0.05, gamma=0.5, runStorey=T)
sig_storeyBH <- get_sig_table(which(route_rejects_storey==1))
print(sig_storeyBH)

# run standard BH on inverse transformation
route_rejects_standard_inverse <- runBH(route_pvals_inverse, alpha=0.05, gamma=0.5, runStorey=F)
sig_BH_inverse <- get_sig_table(which(route_rejects_standard_inverse==1), inverse=T)
print(sig_BH_inverse)

# run on our pvalues w/ Storey correction on inverse information
route_rejects_storey_inverse <- runBH(route_pvals_inverse, alpha=0.05, gamma=0.5, runStorey=T)
sig_storeyBH_inverse <- get_sig_table(which(route_rejects_storey_inverse==1), inverse=T)
print(sig_storeyBH_inverse)
```

```{r}
#save('route_rejects_standard', 'route_rejects_storey', 'route_rejects_standard_inverse', 'route_rejects_storey_inverse', file="data/rejections.RData")
```



```{r}
plot_corrected_route <- function(route_index, inverse=F) {
  route <- routes[mindex,]
  route_data <- data_weather[data_weather$station_start==route$station_start & data_weather$station_end==route$station_end,]
  upper_cutoff <- mean(route_data$duration) + 3 * sd(route_data$duration)
  lower_cutoff <- mean(route_data$duration) - 3 * sd(route_data$duration)
  route_data <- route_data[route_data$duration < upper_cutoff & route_data$duration > lower_cutoff,]
  
  if (!inverse) {
    route_data['duration_resid'] <- resid(lm(duration ~ day_of_week + member + TMAX + PRCP, data=route_data))
  } else {
    route_data['duration_resid'] <- resid(lm((duration^-1) ~ day_of_week + member + TMAX + PRCP, data=route_data))
  }
  
  df <- route_data %>%
    group_by(days_since_Jan1_2010) %>%
    summarize(mean_daily_duration_residualized = mean(duration_resid), count=n(), day_of_week=day_of_week, member=member)
 
  return(ggplot(data=df, aes(x=days_since_Jan1_2010, y=mean_daily_duration_residualized)) +
    geom_line() + geom_smooth(method='lm', formula=y~x))
}

# plot most significant route duration over time with duration corrected for covariates
mindex <- which.min(route_pvals)
p <- plot_corrected_route(mindex)
print(p)

mindex_inverse <- which.min(route_pvals_inverse)
p <- plot_corrected_route(mindex_inverse, inverse=T)
print(p)

```


```{r}
# find source for geographical data (lat & long) regarding stations
locData <- read.csv("https://gist.githubusercontent.com/since1968/e51c0f3d95e67bf49f74/raw/37a6c381df119b7463c8fd33fdfaa06427d9794f/bikeStations.csv")
colnames(stations) <- c("terminalName","locations")

#take those with matching terminalName
newStations<-merge(x=stations,y=locData,by="terminalName",all.x=TRUE)
newStations<-newStations[, c("terminalName", "locations", "lat", "long")]
```

After the merge, some of the locations do not match (excluding those wherein the order of the street names were simply switched such as "Pentagon City Metro / 12th & S Hayes St" vs "12th & Hayes St /  Pentagon City Metro"). For these pairs of locations that do not match at all, we resorted to verifying the latitude and longitude for the location listed in `stations`, a data object provided from the Group Project 1 instructions. Latitude and longitude for these locations were gathered from inputting location names into Google Maps.

The 5 terminals with different locations are: 31000, 31500, 31302, 31609, 31239
```{r}
newStations[1,3] <- 38.85979; newStations[1,4] <- -77.05357
newStations[78,3] <- 38.90567; newStations[78,4] <- -77.04120 
newStations[1,3] <- 38.93465; newStations[1,4] <--77.07246
newStations[1,3] <- 38.91930; newStations[1,4] <- -77.00056
newStations[1,3] <- 38.87863; newStations[1,4] <- -77.02283
```


## Inverse transformation

```{r}

# test regression transformations 
route_data <- get_route_data(routes[1,]$station_start, routes[1,]$station_end)


# route_data <- data_weather[data_weather$station_start==routes[1,]$station_start & data_weather$station_end==routes[1,]$station_end,]
# upper_cutoff <- mean(route_data$duration) + 3 * sd(route_data$duration) # mention data dependent
# lower_cutoff <- mean(route_data$duration) - 3 * sd(route_data$duration)
# route_data <- route_data[route_data$duration < upper_cutoff & route_data$duration > lower_cutoff,]
fit <- lm(duration ~ days_since_Jan1_2010 + day_of_week + member + TMAX + PRCP, data=route_data) # linear regression assumptions
# box cox
bc <- MASS::boxcox(duration ~ days_since_Jan1_2010 + day_of_week + member + TMAX + PRCP, data=route_data)
lambda <- bc$x[which.max(bc$y)]
# new_fit <- lm(((duration^lambda-1)/lambda) ~ days_since_Jan1_2010 + day_of_week + member + TMAX + PRCP, data=route_data)
new_fit <- lm((duration^-1)~ days_since_Jan1_2010 + day_of_week + member + TMAX + PRCP, data=route_data)
# new_fit <- lm(((y^lambda-1)/lambda) ~ x)

summary(new_fit)
par(mfrow = c(2, 2))
plot(new_fit)
```



## Permutation test
if null, permutation doesnt change dist
- if confounding, permutation can change -> deal with this

       
